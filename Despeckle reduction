{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2008381,"sourceType":"datasetVersion","datasetId":1201791}],"dockerImageVersionId":30121,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### SAR Image Despeckling Using a Convolutional Neural by ( 2 Jun 2017  Â·  Puyang Wang, He Zhang, Vishal M. Patel)\n### ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input,Conv2D,ReLU,BatchNormalization,LeakyReLU\nimport os\nfrom skimage.util import random_noise\nimport sys\nimport time\nfrom tqdm.notebook import tqdm\nimport shutil\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_SIZE= (64,64)\nBS=16\nROOT_DIR=\"/kaggle/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparing DATA\n- I'm using a part of this DataSet: sentinel12-image-pairs-segregated-by-terrain\n- From the Agriculture set I choose the optical set s2\n","metadata":{}},{"cell_type":"code","source":"DATASET=os.path.join(ROOT_DIR,'input/sentinel12-image-pairs-segregated-by-terrain/v_2')\nDATA_GEN_INPUT=os.path.join(ROOT_DIR,'DATASET')\n\nif os.path.exists(DATA_GEN_INPUT):\n    shutil.rmtree(DATA_GEN_INPUT)\nos.mkdir(DATA_GEN_INPUT)\n\nsrc=os.path.join(DATASET,\"agri/s2\")\ndst=os.path.join(DATA_GEN_INPUT,\"DATA\")\nos.symlink(src,dst)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing_function(img):\n    return np.float32(img/127.5-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator=tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessing_function)\ntrain_generator=generator.flow_from_directory(DATA_GEN_INPUT,\n                                              target_size=INPUT_SIZE,\n                                              class_mode=None,\n                                              color_mode='grayscale',\n                                              batch_size=BS,\n                                              follow_links=True,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This is how the data looks like","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.imshow(next(train_generator)[0],cmap='gray')\nplt.colorbar()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing Model","metadata":{}},{"cell_type":"code","source":"\ndef test_model(data_generator):\n    img1,img2=next(data_generator)[:2]\n    noise_var=np.random.rand()*.25\n    # noise_var=.3\n    noisy_img1=random_noise(img1,mode='speckle',var=noise_var,clip=True)\n    noisy_img2=random_noise(img2,mode='speckle',var=noise_var,clip=True)\n    noisy_img1=np.expand_dims(noisy_img1,axis=[0,-1])\n    noisy_img2=np.expand_dims(noisy_img2,axis=[0,-1])\n    denoised_img1=model.predict(noisy_img1)\n    denoised_img2=model.predict(noisy_img2)\n    fig,ax=plt.subplots(3,2,figsize=(10,12))\n    mapple=ax[0,0].imshow(img1)\n    plt.colorbar(mapple,ax=ax[0,0])\n    mapple=ax[0,1].imshow(img2)\n    plt.colorbar(mapple,ax=ax[0,1])\n    mapple=ax[1,0].imshow(noisy_img1[0].reshape(INPUT_SIZE))\n    plt.colorbar(mapple,ax=ax[1,0])\n    mapple=ax[1,1].imshow(noisy_img2[0].reshape(INPUT_SIZE))\n    plt.colorbar(mapple,ax=ax[1,1])\n    mapple=ax[2,0].imshow(denoised_img1[0].reshape(INPUT_SIZE))\n    plt.colorbar(mapple,ax=ax[2,0])\n    mapple=ax[2,1].imshow(denoised_img2[0].reshape(INPUT_SIZE))\n    plt.colorbar(mapple,ax=ax[2,1])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Creation","metadata":{}},{"cell_type":"code","source":"def create_model(input_shape=(256,256,1)):\n    input_layer=Input(shape=input_shape)\n    x=Conv2D(filters=64,kernel_size=(3,3),padding='same')(input_layer)\n    x=LeakyReLU(.2)(x)\n#   Here I'm using dialation in convolution layers but in the original paper There are NO dialation used\n    for i in range(1,5):\n        x=Conv2D(filters=64,kernel_size=(3,3),dilation_rate=i,padding='same',)(x)\n        x=BatchNormalization()(x)\n        x=LeakyReLU(.2)(x)\n    for i in range(4,0,-1):\n        x=Conv2D(filters=64,kernel_size=(3,3),dilation_rate=i,padding='same')(x)\n        x=BatchNormalization()(x)\n        x=ReLU()(x)\n    x=Conv2D(filters=1,kernel_size=(3,3),padding='same')(x)\n    x=ReLU()(x) \n    x= tf.keras.layers.Lambda(lambda x:x+tf.constant(1e-7))(x)\n    x=tf.math.divide(input_layer,x)\n\n    x=tf.math.tanh(x)\n    return tf.keras.Model(inputs=input_layer,outputs=x)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The loss function \n-  I'm using MeanSquaredError and total_variation to implement the loss function ","metadata":{}},{"cell_type":"code","source":"MSE=tf.keras.losses.MeanSquaredError(reduction='none')\ndef loss_fn(y_true,y_pred,l_tv=.0002):\n    mse=tf.reduce_sum(MSE(y_true,y_pred))\n    variational_loss=tf.image.total_variation(y_pred)\n    weight_loss = tf.reduce_sum(tf.math.abs(tf.math.divide(1,y_pred+1e-5)))\n    total_loss=mse+l_tv*variational_loss\n    return tf.reduce_mean(total_loss),tf.reduce_mean(mse),tf.reduce_mean(variational_loss)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training ","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef step(noisy_data, clean_data):\n    with tf.GradientTape() as tape:\n        pred = model(noisy_data,training=True) \n        total_loss,loss_euclidian,loss_tv = loss_fn(clean_data, pred)\n        loss=tf.add_n([total_loss],model.losses)\n    grads = tape.gradient(total_loss, model.trainable_weights)\n    opt.apply_gradients(zip(grads, model.trainable_weights))\n    return loss,loss_euclidian,loss_tv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=create_model(list(INPUT_SIZE)+[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_model(train_generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 100 # The paper has trained the model for 2000 epochs\nlr=2e-3\n\nmax_var=.3\n\nopt = tf.keras.optimizers.Nadam(learning_rate=lr) # in the paper Adam optimizer with lr=2e-5 ,beta_1=.5 is used but I found this one converging faster\ntrain_loss=[]\nn_instances=train_generator.n\nnumUpdates = int(n_instances / BS)\n# loop over the number of epochs\nfor epoch in range(0, EPOCHS): \n    # show the current epoch number\n    print(\"[INFO] starting epoch {}/{} , learning_rate {}\".format(\n        epoch + 1, EPOCHS,lr), end=\"\")\n    sys.stdout.flush()\n    epochStart = time.time()\n    loss = 0\n    loss_batch = []\n    for i in tqdm(range(0, numUpdates)):\n        clean_data = next(train_generator)\n#         I Use Speckle Noise with Random Variance you can try a constant variance \n        noisy_data=random_noise(clean_data,mode='speckle',var=np.random.uniform(high=max_var))\n        loss = step(noisy_data,clean_data)\n        loss_batch.append((loss))\n    loss_batch = np.array(loss_batch)\n    loss_batch = np.sum(loss_batch, axis=0) / len(loss_batch)\n    total_loss,loss_euclidian,loss_tv=loss_batch\n    train_loss.append(loss_batch)\n    print('\\nTraining_loss # ', 'total loss: ', float(total_loss),\n          'loss_euclidian: ', float(loss_euclidian),\n          'loss_tv: ', float(loss_tv),)\n    if epoch % 5==0:\n        plt.plot(train_loss)\n        plt.legend(['Total loss','Euclidian loss','Total Variation loss'])\n        plt.show()\n        test_model(train_generator)\n    sys.stdout.flush()\n    # show timing information for the epoch\n    epochEnd = time.time()\n    elapsed = (epochEnd - epochStart) / 60.0\n    print(\"took {:.4} minutes\".format(elapsed))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_model(train_generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}